{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment #5: A sentence embedder\n",
    "Author: Pierre Nugues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will implement a sentence embedder simplified from Reimers and Gurevych's Sentence-BERT: https://arxiv.org/pdf/1908.10084. S-BERT is written in PyTorch and its code is available from GitHub: https://github.com/UKPLab/sentence-transformers\n",
    "\n",
    "The objectives of the assignment are to:\n",
    "* Write a program to embed sentences\n",
    "* Use neural networks with PyTorch\n",
    "* Write a short report of 2 to 3 pages to describe your program.\n",
    "\n",
    "Note: Should your machine be unable to train a model for the whole dataset, then use only a fraction of the dataset such as 10% or less. For this, use the `MINI_CORPUS` constant. See below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "We saw we can vectorize words using a dense representation. We can extend this to documents. This enables us to store the resulting vectors in databases and then use fast algorithms for paragraph or document comparisons such as Faiss: https://github.com/facebookresearch/faiss\n",
    "\n",
    "There are many document vectorization techniques and models are regularly benchmarked, see: https://huggingface.co/spaces/mteb/leaderboard. See also a list of available vector databases here https://db-engines.com/en/ranking/vector+dbms\n",
    "\n",
    "In this lab, you will program two techniques to vectorize documents into dense vectors. You will first implement a baseline technique and then a toy version of SBERT. SBERT is one of the earliest transformer-based document vectorization algorithm: _Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks_ by Reimers and Gurevych (2019) https://arxiv.org/pdf/1908.10084"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of the algorithms\n",
    "Read the Getting Started paragraph of https://github.com/UKPLab/sentence-transformers for an overview.\n",
    "\n",
    "Read the summary of the SBERT paper as well as Sections 1 and 3, _Introduction_ and _Model_. In the triplet objective function, an anchor is a start sample, the positive sample is close to the anchor, while the negative one is different. Considering a language detector, think of a sentence in Swedish as the anchor. A positive sample would be another sentence in Swedish and a negative one could be a sentence in English.\n",
    "\n",
    "In the _Method and program struture_ section of your report, you will summarize these sections in 10 to 15 lines. Note that a three-way softmax classifier is simply a logistic regression with three classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import regex as re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a reduced dataset for the development of your program with `MINI_CORPUS` set to `True`. Once your program is ready, you can train your model on the whole dataset (if you have the time). Set `MINI_CORPUS` to `False` then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MINI_CORPUS = False  # Set the value to True when you develop the program\n",
    "MINI_PERCENTAGE = 0.01  # Percentage of the original dataset.\n",
    "# Depending on your machine, you may even use less than 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets: SNLI\n",
    "As dataset, you will use SNLI. SNLI consists of over 500,000 lines with the text of the pairs and their labels. The authors created the dataset by giving volunteers a sentence (the premise) and asking them to write a second sentence (the hypothesis) that is either definitely true\n",
    "(entailment), that might be true (neutral), or that is definitely false (contradiction).\n",
    "\n",
    "Read the dataset description from this URL https://nlp.stanford.edu/projects/snli/ and download it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please adjust the path to fit your machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('../tentan2023/snli_1.0/snli_1.0_train.jsonl', 'r') as f:\n",
    "    dataset_list = list(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_snli = []\n",
    "for json_str in dataset_list:\n",
    "    dataset_snli += [json.loads(json_str)]\n",
    "    # print(f\"result: {result}\")\n",
    "    # print(isinstance(result, dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sample with an agreement in the annotation. The final annotation is the gold label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'annotator_labels': ['entailment'],\n",
       " 'captionID': '3706019259.jpg#3',\n",
       " 'gold_label': 'entailment',\n",
       " 'pairID': '3706019259.jpg#3r2e',\n",
       " 'sentence1': 'A foreign family is walking along a dirt path next to the water.',\n",
       " 'sentence1_binary_parse': '( ( A ( foreign family ) ) ( ( is ( ( walking ( along ( a ( dirt path ) ) ) ) ( next ( to ( the water ) ) ) ) ) . ) )',\n",
       " 'sentence1_parse': '(ROOT (S (NP (DT A) (JJ foreign) (NN family)) (VP (VBZ is) (VP (VBG walking) (PP (IN along) (NP (DT a) (NN dirt) (NN path))) (ADVP (JJ next) (PP (TO to) (NP (DT the) (NN water)))))) (. .)))',\n",
       " 'sentence2': 'A family of foreigners walks by the water.',\n",
       " 'sentence2_binary_parse': '( ( ( A family ) ( of foreigners ) ) ( ( walks ( by ( the water ) ) ) . ) )',\n",
       " 'sentence2_parse': '(ROOT (S (NP (NP (DT A) (NN family)) (PP (IN of) (NP (NNS foreigners)))) (VP (VBZ walks) (PP (IN by) (NP (DT the) (NN water)))) (. .)))'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_snli[300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sample with no agreement in the annotation. The gold label is `_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'annotator_labels': ['contradiction', 'contradiction', 'neutral', 'neutral'],\n",
       " 'captionID': '2677109430.jpg#2',\n",
       " 'gold_label': '-',\n",
       " 'pairID': '2677109430.jpg#2r1c',\n",
       " 'sentence1': 'A small group of church-goers watch a choir practice.',\n",
       " 'sentence1_binary_parse': '( ( ( A ( small group ) ) ( of church-goers ) ) ( ( watch ( a ( choir practice ) ) ) . ) )',\n",
       " 'sentence1_parse': '(ROOT (S (NP (NP (DT A) (JJ small) (NN group)) (PP (IN of) (NP (NNS church-goers)))) (VP (VBP watch) (NP (DT a) (NN choir) (NN practice))) (. .)))',\n",
       " 'sentence2': 'A choir performs in front of packed crowd.',\n",
       " 'sentence2_binary_parse': '( ( A choir ) ( ( performs ( in ( front ( of ( packed crowd ) ) ) ) ) . ) )',\n",
       " 'sentence2_parse': '(ROOT (S (NP (DT A) (NN choir)) (VP (VBZ performs) (PP (IN in) (NP (NP (NN front)) (PP (IN of) (NP (JJ packed) (NN crowd)))))) (. .)))'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_snli[145]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove all the samples that have no agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_str = []\n",
    "for sample in dataset_snli:\n",
    "    s1 = sample['sentence1']\n",
    "    s2 = sample['sentence2']\n",
    "    label = sample['gold_label']\n",
    "    if label != '-':\n",
    "        dataset_str += [(s1, s2, label)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "549367"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('A person on a horse jumps over a broken down airplane.',\n",
       " 'A person is training his horse for a competition.',\n",
       " 'neutral')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('A person on a horse jumps over a broken down airplane.',\n",
       " 'A person is at a diner, ordering an omelette.',\n",
       " 'contradiction')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_str[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('A person on a horse jumps over a broken down airplane.',\n",
       " 'A person is outdoors, on a horse.',\n",
       " 'entailment')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_str[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MINI_CORPUS:\n",
    "    new_size = int(len(dataset_str) * MINI_PERCENTAGE)\n",
    "    dataset_str = dataset_str[:new_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "549367"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets: GloVe\n",
    "\n",
    "You will first implement a baseline, an easy technique that serves as comparison for more elaborate ones. \n",
    "\n",
    "In Sect. 4.1 and Table 1 the authors proposed a baseline technique for computing a semantic\n",
    "textual similarity between two sentences that uses GloVe embeddings. Describe this technique in 5 to 10 lines in the _Method and program struture_ section. You will create a *Baseline* subsection for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe\n",
    "You will use a list of pretrained word embeddings to implement the baseline and GloVe is one such vector lists. GloVe is available in different dimensionalities (50, 100, 200, 300) and vocabulary sizes (400,000 words, 1.2M, 2.4M). \n",
    "\n",
    "Download the GloVe 6B embeddings from https://nlp.stanford.edu/projects/glove/, uncompress it, and keep the `glove.6B.50d.txt` file of 400,000 words with 50-dimensional vectors.\n",
    "\n",
    "Please adjust your path to read the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_file = '../../corpus/glove.6B.50d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_embeddings(file):\n",
    "    \"\"\"\n",
    "    Return the embeddings in the from of a dictionary\n",
    "    :param file:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    embeddings = {}\n",
    "    glove = open(file, encoding='utf8')\n",
    "    for line in glove:\n",
    "        values = line.strip().split()\n",
    "        word = values[0]\n",
    "        vector = torch.tensor(\n",
    "            list(map(float, values[1:])), dtype=torch.float32)\n",
    "        embeddings[word] = vector\n",
    "    glove.close()\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = read_embeddings(embedding_file)\n",
    "embedded_words = sorted(list(embeddings.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4.1800e-01,  2.4968e-01, -4.1242e-01,  1.2170e-01,  3.4527e-01,\n",
       "        -4.4457e-02, -4.9688e-01, -1.7862e-01, -6.6023e-04, -6.5660e-01,\n",
       "         2.7843e-01, -1.4767e-01, -5.5677e-01,  1.4658e-01, -9.5095e-03,\n",
       "         1.1658e-02,  1.0204e-01, -1.2792e-01, -8.4430e-01, -1.2181e-01,\n",
       "        -1.6801e-02, -3.3279e-01, -1.5520e-01, -2.3131e-01, -1.9181e-01,\n",
       "        -1.8823e+00, -7.6746e-01,  9.9051e-02, -4.2125e-01, -1.9526e-01,\n",
       "         4.0071e+00, -1.8594e-01, -5.2287e-01, -3.1681e-01,  5.9213e-04,\n",
       "         7.4449e-03,  1.7778e-01, -1.5897e-01,  1.2041e-02, -5.4223e-02,\n",
       "        -2.9871e-01, -1.5749e-01, -3.4758e-01, -4.5637e-02, -4.4251e-01,\n",
       "         1.8785e-01,  2.7849e-03, -1.8411e-01, -1.1514e-01, -7.8581e-01])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model = next(iter(embeddings.values())).size()[0]\n",
    "d_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read all the words in GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_words = []\n",
    "glove = []\n",
    "for word, vector in embeddings.items():\n",
    "    glove_words += [word]\n",
    "    glove += [vector]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', ',', '.', 'of', 'to', 'and', 'in', 'a', '\"', \"'s\"]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we create a tensor with the vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = torch.stack(glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4.1800e-01,  2.4968e-01, -4.1242e-01,  1.2170e-01,  3.4527e-01,\n",
       "        -4.4457e-02, -4.9688e-01, -1.7862e-01, -6.6023e-04, -6.5660e-01,\n",
       "         2.7843e-01, -1.4767e-01, -5.5677e-01,  1.4658e-01, -9.5095e-03,\n",
       "         1.1658e-02,  1.0204e-01, -1.2792e-01, -8.4430e-01, -1.2181e-01,\n",
       "        -1.6801e-02, -3.3279e-01, -1.5520e-01, -2.3131e-01, -1.9181e-01,\n",
       "        -1.8823e+00, -7.6746e-01,  9.9051e-02, -4.2125e-01, -1.9526e-01,\n",
       "         4.0071e+00, -1.8594e-01, -5.2287e-01, -3.1681e-01,  5.9213e-04,\n",
       "         7.4449e-03,  1.7778e-01, -1.5897e-01,  1.2041e-02, -5.4223e-02,\n",
       "        -2.9871e-01, -1.5749e-01, -3.4758e-01, -4.5637e-02, -4.4251e-01,\n",
       "         1.8785e-01,  2.7849e-03, -1.8411e-01, -1.1514e-01, -7.8581e-01])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove[0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of word embeddings (400,000) and their dimension (50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([400000, 50])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model = glove.size()[1]\n",
    "d_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reserve three special symbols: padding, unknown, and the first classification token of BERT. See the lecture on transformers for a clarification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "specials = ['[PAD]', '[UNK]', '[CLS]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[PAD]', '[UNK]', '[CLS]', 'the', ',', '.', 'of', 'to', 'and', 'in']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_words = specials + glove_words\n",
    "glove_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add the vectors for the special tokens to our tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = torch.vstack((torch.zeros((3, d_model)), glove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([400003, 50])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0.])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4.1800e-01,  2.4968e-01, -4.1242e-01,  1.2170e-01,  3.4527e-01,\n",
       "        -4.4457e-02, -4.9688e-01, -1.7862e-01, -6.6023e-04, -6.5660e-01,\n",
       "         2.7843e-01, -1.4767e-01, -5.5677e-01,  1.4658e-01, -9.5095e-03,\n",
       "         1.1658e-02,  1.0204e-01, -1.2792e-01, -8.4430e-01, -1.2181e-01,\n",
       "        -1.6801e-02, -3.3279e-01, -1.5520e-01, -2.3131e-01, -1.9181e-01,\n",
       "        -1.8823e+00, -7.6746e-01,  9.9051e-02, -4.2125e-01, -1.9526e-01,\n",
       "         4.0071e+00, -1.8594e-01, -5.2287e-01, -3.1681e-01,  5.9213e-04,\n",
       "         7.4449e-03,  1.7778e-01, -1.5897e-01,  1.2041e-02, -5.4223e-02,\n",
       "        -2.9871e-01, -1.5749e-01, -3.4758e-01, -4.5637e-02, -4.4251e-01,\n",
       "         1.8785e-01,  2.7849e-03, -1.8411e-01, -1.1514e-01, -7.8581e-01])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove[3, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "### Tokenization\n",
    "You will now tokenize the sentences\n",
    "\n",
    "Write a regular expression that tokenizes the words, numbers, and punctuation. Use Unicode classes. Note that a punctuation is a single symbol while the words and numbers are sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "pattern = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a tokenization function that takes a string as input and results a list of tokens. Set the string in lower case by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "def tokenize(sentence, pattern, lc=True):\n",
    "    if lc:\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'person',\n",
       " 'on',\n",
       " 'a',\n",
       " 'horse',\n",
       " 'jumps',\n",
       " 'over',\n",
       " 'a',\n",
       " 'broken',\n",
       " 'down',\n",
       " 'airplane',\n",
       " '.']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(dataset_str[0][0], pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a code that, for each sample of your dataset, builds a triple consisting of:\n",
    "1. The first tokenized sentence, \n",
    "2. The second one, and \n",
    "3. The class\n",
    "\n",
    "Build a list of all these triples to represent your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "dataset_tokens = []\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['a',\n",
       "  'person',\n",
       "  'on',\n",
       "  'a',\n",
       "  'horse',\n",
       "  'jumps',\n",
       "  'over',\n",
       "  'a',\n",
       "  'broken',\n",
       "  'down',\n",
       "  'airplane',\n",
       "  '.'],\n",
       " ['a',\n",
       "  'person',\n",
       "  'is',\n",
       "  'training',\n",
       "  'his',\n",
       "  'horse',\n",
       "  'for',\n",
       "  'a',\n",
       "  'competition',\n",
       "  '.'],\n",
       " 'neutral')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "549367"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build token-to-index `token2idx` and index-to-token `idx2token` dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 10)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2idx['the'], token2idx['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('the', 'a')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2token[3], idx2token[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect the set of all the labels (classes) from your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "labels = []\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['entailment', 'neutral', 'contradiction']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build label-to-index `label2idx` and index-to-label `idx2label` dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entailment': 0, 'neutral': 1, 'contradiction': 2}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'entailment', 1: 'neutral', 2: 'contradiction'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to convert:\n",
    "  * A list of tokens into a list of `LongTensor` indices and \n",
    "  * The class to a tensor. \n",
    "\n",
    "Your function should be able to handle two types: either a list or a string. The tokens are strored in a list and the class (label) is a string\n",
    "\n",
    "Note that an unknown token in GloVe should be mapped to the `UNK` symbol of index 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "def convert_symbols(symbols, symbol2idx):\n",
    "    if type(symbols) is str:\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['a',\n",
       "  'person',\n",
       "  'on',\n",
       "  'a',\n",
       "  'horse',\n",
       "  'jumps',\n",
       "  'over',\n",
       "  'a',\n",
       "  'broken',\n",
       "  'down',\n",
       "  'airplane',\n",
       "  '.'],\n",
       " ['a',\n",
       "  'person',\n",
       "  'is',\n",
       "  'training',\n",
       "  'his',\n",
       "  'horse',\n",
       "  'for',\n",
       "  'a',\n",
       "  'competition',\n",
       "  '.'],\n",
       " 'neutral')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_tokens[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert a list of tokens into a `LongTensor` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   10,   902,    16,    10,  2870, 11073,    77,    10,  2324,   138,\n",
       "         7353,     5])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_symbols(dataset_tokens[0][0], token2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  10,  902,   17,  791,   29, 2870,   13,   10,  994,    5])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_symbols(dataset_tokens[0][1], token2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neutral'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_tokens[0][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert a label string into a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_symbols(dataset_tokens[0][2], label2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unknown tokens have the index 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'person', 'on', 'a', 'horsewww']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize('a person on a horsewww', pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 10, 902,  16,  10,   1,   1])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_symbols(tokenize('a person on a horsewww wxwx', pattern), token2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the tokens and labels in your dataset by their indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "dataset = []\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([   10,   902,    16,    10,  2870, 11073,    77,    10,  2324,   138,\n",
       "          7353,     5]),\n",
       " tensor([    10,    902,     17,     25,     10,  19304,      4,   7490,     32,\n",
       "         119031,      5]),\n",
       " tensor(2))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch `Embedding` class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now store the GloVe vectors in a PyTorch `Embedding` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embs = nn.Embedding(glove.size()[0],\n",
    "                          glove.size()[1],\n",
    "                          padding_idx=0).from_pretrained(glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We access the embedding for _the_ with its index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.1800e-01,  2.4968e-01, -4.1242e-01,  1.2170e-01,  3.4527e-01,\n",
       "         -4.4457e-02, -4.9688e-01, -1.7862e-01, -6.6023e-04, -6.5660e-01,\n",
       "          2.7843e-01, -1.4767e-01, -5.5677e-01,  1.4658e-01, -9.5095e-03,\n",
       "          1.1658e-02,  1.0204e-01, -1.2792e-01, -8.4430e-01, -1.2181e-01,\n",
       "         -1.6801e-02, -3.3279e-01, -1.5520e-01, -2.3131e-01, -1.9181e-01,\n",
       "         -1.8823e+00, -7.6746e-01,  9.9051e-02, -4.2125e-01, -1.9526e-01,\n",
       "          4.0071e+00, -1.8594e-01, -5.2287e-01, -3.1681e-01,  5.9213e-04,\n",
       "          7.4449e-03,  1.7778e-01, -1.5897e-01,  1.2041e-02, -5.4223e-02,\n",
       "         -2.9871e-01, -1.5749e-01, -3.4758e-01, -4.5637e-02, -4.4251e-01,\n",
       "          1.8785e-01,  2.7849e-03, -1.8411e-01, -1.1514e-01, -7.8581e-01]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_embs(torch.LongTensor([3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: Mean of GloVe Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('A person on a horse jumps over a broken down airplane.',\n",
       " 'A person is training his horse for a competition.',\n",
       " 'neutral')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1_str = dataset_str[0][0]\n",
    "s2_str = dataset_str[0][1]\n",
    "s3_str = dataset_str[0][2]\n",
    "s1_str, s2_str, s3_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([   10,   902,    16,    10,  2870, 11073,    77,    10,  2324,   138,\n",
       "          7353,     5]),\n",
       " tensor([  10,  902,   17,  791,   29, 2870,   13,   10,  994,    5]),\n",
       " tensor(1))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1_idx = dataset[0][0]\n",
    "s2_idx = dataset[0][1]\n",
    "s3_idx = dataset[0][2]\n",
    "s1_idx, s2_idx, s3_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that takes a list of indices and the GloVe embeddings as input and that computes the mean of the word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "def mean_embs(input_idx: torch.LongTensor, glove_embs: nn.Embedding) -> torch.tensor:\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([   10,   902,    16,    10,  2870, 11073,    77,    10,  2324,   138,\n",
       "          7353,     5]),\n",
       " tensor([  10,  902,   17,  791,   29, 2870,   13,   10,  994,    5]),\n",
       " tensor(1))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1988,  0.1043,  0.0180, -0.0851,  0.5251,  0.4551, -0.4729, -0.0604,\n",
       "         0.1335, -0.1824, -0.1023, -0.2145, -0.3953,  0.3100,  0.3126, -0.1235,\n",
       "        -0.1631,  0.1271, -0.8334, -0.5111,  0.0911,  0.1766, -0.1190, -0.1795,\n",
       "         0.2117, -1.6935, -0.1754,  0.3900,  0.4590, -0.1137,  3.0905, -0.0358,\n",
       "        -0.2404,  0.2918,  0.1015, -0.0099,  0.2168,  0.1239,  0.1565, -0.2061,\n",
       "        -0.1449,  0.0871, -0.1085,  0.1992, -0.0306, -0.2125,  0.1155, -0.3489,\n",
       "         0.2139, -0.1993])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_embs(dataset[0][0], glove_embs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to compute the cosine of two vectors. You will return `torch.tensor(0.0)` if one of the vectors is zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "def compute_cosine(v1: torch.tensor, v2: torch.tensor) -> torch.tensor:\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.1988,  0.1043,  0.0180, -0.0851,  0.5251,  0.4551, -0.4729, -0.0604,\n",
       "          0.1335, -0.1824, -0.1023, -0.2145, -0.3953,  0.3100,  0.3126, -0.1235,\n",
       "         -0.1631,  0.1271, -0.8334, -0.5111,  0.0911,  0.1766, -0.1190, -0.1795,\n",
       "          0.2117, -1.6935, -0.1754,  0.3900,  0.4590, -0.1137,  3.0905, -0.0358,\n",
       "         -0.2404,  0.2918,  0.1015, -0.0099,  0.2168,  0.1239,  0.1565, -0.2061,\n",
       "         -0.1449,  0.0871, -0.1085,  0.1992, -0.0306, -0.2125,  0.1155, -0.3489,\n",
       "          0.2139, -0.1993]),\n",
       " tensor([ 1.0878e-01,  3.7231e-01, -4.7114e-01, -1.3591e-02,  5.1340e-01,\n",
       "          3.7193e-01, -4.4592e-01, -5.9900e-02,  2.5352e-01, -1.3076e-01,\n",
       "          1.6231e-01,  2.3146e-03, -3.6474e-01,  2.3840e-03,  3.4653e-01,\n",
       "         -2.1769e-01,  2.5946e-02,  3.9537e-01, -6.9517e-01, -3.4811e-01,\n",
       "         -4.9454e-02,  1.4977e-01, -1.2447e-01,  8.1851e-02,  6.2581e-02,\n",
       "         -1.8692e+00, -3.1502e-01, -7.4079e-02,  1.2478e-01, -1.6717e-02,\n",
       "          3.3707e+00,  8.7725e-02, -4.0180e-01, -1.8131e-01,  2.5315e-01,\n",
       "          1.7589e-01,  2.2877e-01,  4.3286e-01, -1.6315e-02, -3.6988e-01,\n",
       "         -2.8208e-02,  3.1538e-02, -2.4721e-01,  2.5963e-01,  1.3792e-02,\n",
       "         -2.6431e-01,  1.7081e-02, -2.0042e-01,  1.6257e-01,  2.1471e-01]))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = mean_embs(s1_idx, glove_embs)\n",
    "v2 = mean_embs(s2_idx, glove_embs)\n",
    "v1, v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9426)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_cosine(v1, v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compute the cosine of pairs for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 549367/549367 [00:14<00:00, 38384.96it/s]\n"
     ]
    }
   ],
   "source": [
    "cos_sim = {'entailment': 0.0, 'neutral': 0.0, 'contradiction': 0.0}\n",
    "cnt = {'entailment': 0.0, 'neutral': 0.0, 'contradiction': 0.0}\n",
    "for data in tqdm(dataset):\n",
    "    cos_val = compute_cosine(\n",
    "        mean_embs(data[0], glove_embs),\n",
    "        mean_embs(data[1], glove_embs))\n",
    "    class_name = idx2label[data[2].item()]\n",
    "    cos_sim[class_name] += cos_val\n",
    "    cnt[class_name] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will comment these values in your report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entailment': tensor(0.9477),\n",
       " 'neutral': tensor(0.9401),\n",
       " 'contradiction': tensor(0.9317)}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for key in cos_sim.keys():\n",
    "    cos_sim[key] /= cnt[key]\n",
    "cos_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SBERT: The Stacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now create the SBERT architecture and replicate the pipeline in Fig. 1 in the paper. In the next cells, we walk through the figure from the bottom to the top."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first layer builds an input consisting of two sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([   3,  360, 5453]), tensor([   3,  194,  368, 2929]))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1 = torch.LongTensor(\n",
    "    list(map(lambda x: token2idx.get(x, 1), tokenize('the small cat', pattern))))\n",
    "p2 = torch.LongTensor(\n",
    "    list(map(lambda x: token2idx.get(x, 1), tokenize('the very big dog', pattern))))\n",
    "p1, p2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we have the BERT layer. Using PyTorch classes, create an encoder of four layers where each layer has five heads. You will use the classes `TransformerEncoderLayer` and `TransformerEncoder`. The dimensionality `d_model` is 50 as this is the size of GloVe vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.13/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Write your code\n",
    "d_model = 50\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerEncoder(\n",
       "  (layers): ModuleList(\n",
       "    (0-3): 4 x TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=50, out_features=50, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=50, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=50, bias=True)\n",
       "      (norm1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now a BERT encoder. We associate each input index to an embedding. In the next cell, we create three embedding vectors correponding to three words.\n",
    "\n",
    "In the rest of the program, all our batches will have only one sample to eliminate the need for padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8.6657e-01, 6.6400e-05, 9.7701e-01, 1.0455e-01, 4.9846e-01, 7.2580e-01,\n",
       "         7.6163e-01, 9.5108e-01, 2.1879e-01, 2.8177e-01, 4.5431e-01, 7.5656e-01,\n",
       "         5.8611e-01, 8.8444e-02, 6.9580e-01, 4.5230e-01, 2.7514e-01, 3.9332e-01,\n",
       "         8.1388e-01, 2.6017e-01, 9.2815e-01, 5.3411e-01, 3.0771e-02, 2.1434e-02,\n",
       "         2.4295e-01, 5.8212e-01, 3.8342e-01, 7.1828e-01, 5.9748e-01, 8.1659e-01,\n",
       "         6.4132e-01, 2.5315e-02, 9.1239e-01, 5.7229e-01, 5.1909e-01, 5.7530e-01,\n",
       "         9.6192e-01, 9.6495e-01, 1.1372e-02, 4.0569e-02, 4.7995e-01, 1.2995e-01,\n",
       "         9.8711e-01, 3.4735e-01, 6.1290e-01, 2.9465e-01, 3.5841e-01, 9.0846e-01,\n",
       "         1.4531e-01, 5.0160e-01],\n",
       "        [5.9974e-01, 1.0247e-01, 8.6797e-01, 9.1659e-01, 1.6542e-01, 7.5023e-01,\n",
       "         9.9609e-01, 1.4233e-01, 1.8950e-01, 1.7897e-01, 7.8687e-01, 1.0258e-01,\n",
       "         6.3434e-01, 9.1244e-01, 9.3878e-01, 3.8194e-01, 9.1965e-01, 9.1795e-01,\n",
       "         8.7072e-01, 9.3136e-01, 4.4033e-01, 3.0105e-01, 5.6160e-01, 2.6417e-01,\n",
       "         3.6825e-02, 4.8642e-01, 7.3717e-01, 5.2823e-01, 5.6449e-01, 7.6576e-01,\n",
       "         4.9436e-01, 7.1915e-01, 3.5831e-01, 9.7873e-01, 3.2793e-01, 7.2259e-01,\n",
       "         2.4757e-01, 6.1839e-01, 1.1231e-01, 4.4456e-02, 1.5227e-01, 1.2884e-01,\n",
       "         4.6638e-02, 7.6161e-01, 5.5778e-01, 7.1925e-01, 1.7098e-01, 1.1715e-02,\n",
       "         9.5423e-01, 4.8739e-01],\n",
       "        [1.3936e-02, 4.6417e-01, 5.1694e-01, 7.4874e-01, 7.4948e-01, 1.0428e-02,\n",
       "         4.1240e-01, 2.6220e-01, 9.9604e-01, 5.6580e-01, 8.8692e-01, 5.4419e-01,\n",
       "         1.6003e-01, 1.0723e-01, 8.3840e-01, 2.9848e-01, 3.2285e-01, 3.8876e-02,\n",
       "         4.6310e-01, 6.2764e-05, 3.5419e-01, 2.1151e-01, 5.1448e-03, 8.1786e-01,\n",
       "         7.0020e-01, 9.3386e-01, 7.3031e-02, 4.1407e-01, 7.3840e-01, 1.3411e-01,\n",
       "         6.0790e-01, 7.5820e-01, 4.0965e-01, 9.7457e-01, 7.2655e-01, 8.8200e-02,\n",
       "         1.5756e-01, 8.2834e-01, 8.8528e-01, 7.5393e-01, 7.8925e-01, 4.8765e-01,\n",
       "         8.7768e-01, 3.3655e-01, 3.8120e-01, 9.6703e-01, 8.1012e-02, 3.2941e-01,\n",
       "         7.2476e-01, 7.4638e-01]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src = torch.rand(3, d_model)\n",
    "src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pass it to the encoder to encode the input into three vectors of the same size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0071, -2.1203, -0.6910, -0.0923, -1.0608, -0.5878,  0.4240, -0.0446,\n",
       "          0.0828, -0.0097,  1.3393,  0.7339, -0.8317,  1.1654, -0.1915, -0.3266,\n",
       "          0.9081,  0.0365, -0.4389, -2.0919,  0.7220,  0.5317,  1.1851, -0.2911,\n",
       "          1.4803,  0.4806,  2.2001,  1.1610, -0.4237,  0.9493,  1.6908, -0.9204,\n",
       "         -0.2297,  0.7927,  0.4498,  0.6469,  1.4402, -0.6310,  0.6328, -1.7049,\n",
       "         -1.2126, -1.8729,  1.2193, -0.2066, -0.5394,  0.0469, -0.8540, -1.2965,\n",
       "         -0.6640,  0.0216],\n",
       "        [-0.7345, -1.3119, -0.6657,  0.9297, -1.1747, -0.6083,  0.3261, -0.9734,\n",
       "          0.5028, -0.8986,  2.2334,  0.3270, -1.2190,  0.8086, -0.7903, -1.2254,\n",
       "          1.0634,  1.6864,  1.0296, -1.5847,  0.4305, -0.4571,  2.0041, -0.0617,\n",
       "          0.8697,  0.0493,  1.8174,  0.1477, -0.3148,  0.5786,  1.4357,  0.3232,\n",
       "         -0.4752,  1.5678,  0.0144,  0.4011,  0.1771, -1.0430,  0.9238, -1.0251,\n",
       "         -1.0592, -1.5095,  0.0852, -0.4600, -0.8095,  0.8895, -0.5289, -1.7305,\n",
       "         -0.4009,  0.4397],\n",
       "        [-1.8404, -1.4468, -0.6755, -0.3707, -0.3831, -1.0214, -0.1900, -1.6559,\n",
       "          1.2479,  0.1743,  2.0129,  1.2095, -1.3285,  0.1889, -0.7193, -1.0909,\n",
       "          1.0729,  0.3440, -0.0728, -1.5971, -0.4469,  0.4687,  0.4170,  0.5547,\n",
       "          1.3189,  0.3065,  1.2740,  0.3302, -0.4657,  0.0549,  1.7564,  0.3539,\n",
       "         -0.4894,  0.7547,  0.3067,  0.2215, -0.5407, -0.0305,  2.0474, -0.7935,\n",
       "         -0.5505, -0.9033,  1.5750,  0.1950, -0.7664,  1.2085,  0.1208, -1.5847,\n",
       "         -1.5395,  0.9882]], grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = transformer_encoder(src)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a statement that will compute the mean of these embeddings. You will use the `mean(dim)` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.1940, -1.6263, -0.6774,  0.1556, -0.8729, -0.7392,  0.1867, -0.8913,\n",
       "         0.6112, -0.2447,  1.8618,  0.7568, -1.1264,  0.7210, -0.5670, -0.8810,\n",
       "         1.0148,  0.6890,  0.1726, -1.7579,  0.2352,  0.1811,  1.2020,  0.0673,\n",
       "         1.2230,  0.2788,  1.7638,  0.5463, -0.4014,  0.5276,  1.6276, -0.0811,\n",
       "        -0.3981,  1.0384,  0.2569,  0.4232,  0.3589, -0.5682,  1.2013, -1.1745,\n",
       "        -0.9408, -1.4286,  0.9598, -0.1572, -0.7051,  0.7150, -0.4207, -1.5372,\n",
       "        -0.8681,  0.4832], grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write your code\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have understood the first steps of SBERT, we can implement them in a class.\n",
    "\n",
    "In the next cell, write the `forward()` method that takes the two sentences as input in the form of two `LongTensor` of indices.\n",
    "1. Extract their embeddings from the GloVe embedding matrix.\n",
    "2. Encode them with the transformer, and \n",
    "3. Compute their respective mean. You will call these vectors $\\mathbf{u}$ and $\\mathbf{v}$\n",
    "4. Return these two vectors of means. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the forward method\n",
    "class SBERT(nn.Module):\n",
    "    def __init__(self, nbr_classes, glove, d_model=50, nhead=5, num_layers=4):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embeddings = nn.Embedding(\n",
    "            glove.size()[0],\n",
    "            glove.size()[1],\n",
    "            padding_idx=0).from_pretrained(glove)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model, nhead)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            self.encoder_layer, num_layers=num_layers)\n",
    "        # We do not use this last line for now\n",
    "        self.fc = nn.Linear(3 * d_model, nbr_classes)\n",
    "\n",
    "    def forward(self, u, v):\n",
    "        ...\n",
    "        return u, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert = SBERT(3, glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SBERT(\n",
       "  (embeddings): Embedding(400003, 50)\n",
       "  (encoder_layer): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=50, out_features=50, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=50, out_features=2048, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=2048, out_features=50, bias=True)\n",
       "    (norm1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=50, out_features=50, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=50, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=50, bias=True)\n",
       "        (norm1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=150, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbert.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([   10,   902,    16,    10,  2870, 11073,    77,    10,  2324,   138,\n",
       "          7353,     5]),\n",
       " tensor([  10,  902,   17,  791,   29, 2870,   13,   10,  994,    5]),\n",
       " tensor(1))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, v = sbert(dataset[0][0], dataset[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.5450, -1.3992,  0.3945,  0.2541,  0.0563, -0.0975, -1.1654, -1.4632,\n",
       "         -0.2173,  0.4266, -0.2946,  1.3586, -0.8091,  2.1900, -0.0389,  0.9053,\n",
       "          0.5480,  0.1451, -0.9766, -0.4610, -1.6035, -0.0899, -0.0649,  0.3617,\n",
       "         -0.8875, -0.7915, -1.5489,  0.4577,  0.1632,  0.6182,  2.1066, -0.0907,\n",
       "         -0.0325, -0.1371, -1.3814,  0.3233, -1.9433,  0.6890, -1.0303, -1.5014,\n",
       "          0.8300,  2.2735,  0.7271,  1.6309, -0.2483,  0.2067,  0.5331,  0.1963,\n",
       "          0.2378,  0.0956], grad_fn=<MeanBackward1>),\n",
       " tensor([ 5.1262e-01, -1.1404e+00, -7.3076e-02,  2.9437e-01,  6.5641e-02,\n",
       "         -8.0406e-02, -1.1343e+00, -1.0197e+00, -3.1126e-01,  4.5107e-01,\n",
       "         -3.6338e-01,  1.1898e+00, -5.3663e-01,  2.0537e+00, -4.4856e-01,\n",
       "          8.5633e-01,  7.6132e-01,  3.1084e-01, -1.0927e+00, -4.6832e-01,\n",
       "         -1.5766e+00, -6.8404e-02, -2.5635e-01,  4.6666e-01, -9.5128e-01,\n",
       "         -8.8700e-01, -1.5697e+00,  3.4205e-01,  2.2996e-01,  5.6984e-01,\n",
       "          2.4833e+00,  8.1715e-03,  4.7356e-02, -2.2982e-01, -1.0976e+00,\n",
       "          3.5158e-01, -2.0693e+00,  6.7443e-01, -1.1339e+00, -1.6846e+00,\n",
       "          8.1322e-01,  2.2995e+00,  5.9971e-01,  1.6798e+00,  2.4695e-03,\n",
       "          2.6433e-02,  6.9625e-01,  3.4558e-01, -6.0699e-02,  1.2209e-01],\n",
       "        grad_fn=<MeanBackward1>))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u, v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the code that concatenate `u`, `v`, and `|u-v|`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5.4497e-01, -1.3992e+00,  3.9453e-01,  2.5409e-01,  5.6272e-02,\n",
       "        -9.7519e-02, -1.1654e+00, -1.4632e+00, -2.1735e-01,  4.2660e-01,\n",
       "        -2.9465e-01,  1.3586e+00, -8.0910e-01,  2.1900e+00, -3.8915e-02,\n",
       "         9.0532e-01,  5.4804e-01,  1.4513e-01, -9.7657e-01, -4.6097e-01,\n",
       "        -1.6035e+00, -8.9918e-02, -6.4870e-02,  3.6169e-01, -8.8754e-01,\n",
       "        -7.9146e-01, -1.5489e+00,  4.5773e-01,  1.6324e-01,  6.1815e-01,\n",
       "         2.1066e+00, -9.0666e-02, -3.2509e-02, -1.3713e-01, -1.3814e+00,\n",
       "         3.2330e-01, -1.9433e+00,  6.8900e-01, -1.0303e+00, -1.5014e+00,\n",
       "         8.3004e-01,  2.2735e+00,  7.2711e-01,  1.6309e+00, -2.4835e-01,\n",
       "         2.0668e-01,  5.3310e-01,  1.9626e-01,  2.3778e-01,  9.5610e-02,\n",
       "         5.1262e-01, -1.1404e+00, -7.3076e-02,  2.9437e-01,  6.5641e-02,\n",
       "        -8.0406e-02, -1.1343e+00, -1.0197e+00, -3.1126e-01,  4.5107e-01,\n",
       "        -3.6338e-01,  1.1898e+00, -5.3663e-01,  2.0537e+00, -4.4856e-01,\n",
       "         8.5633e-01,  7.6132e-01,  3.1084e-01, -1.0927e+00, -4.6832e-01,\n",
       "        -1.5766e+00, -6.8404e-02, -2.5635e-01,  4.6666e-01, -9.5128e-01,\n",
       "        -8.8700e-01, -1.5697e+00,  3.4205e-01,  2.2996e-01,  5.6984e-01,\n",
       "         2.4833e+00,  8.1715e-03,  4.7356e-02, -2.2982e-01, -1.0976e+00,\n",
       "         3.5158e-01, -2.0693e+00,  6.7443e-01, -1.1339e+00, -1.6846e+00,\n",
       "         8.1322e-01,  2.2995e+00,  5.9971e-01,  1.6798e+00,  2.4695e-03,\n",
       "         2.6433e-02,  6.9625e-01,  3.4558e-01, -6.0699e-02,  1.2209e-01,\n",
       "         3.2344e-02,  2.5883e-01,  4.6761e-01,  4.0286e-02,  9.3694e-03,\n",
       "         1.7114e-02,  3.1098e-02,  4.4345e-01,  9.3912e-02,  2.4468e-02,\n",
       "         6.8733e-02,  1.6883e-01,  2.7247e-01,  1.3628e-01,  4.0964e-01,\n",
       "         4.8988e-02,  2.1328e-01,  1.6571e-01,  1.1613e-01,  7.3536e-03,\n",
       "         2.6894e-02,  2.1513e-02,  1.9148e-01,  1.0497e-01,  6.3736e-02,\n",
       "         9.5537e-02,  2.0706e-02,  1.1568e-01,  6.6722e-02,  4.8311e-02,\n",
       "         3.7662e-01,  9.8837e-02,  7.9865e-02,  9.2686e-02,  2.8377e-01,\n",
       "         2.8284e-02,  1.2599e-01,  1.4570e-02,  1.0353e-01,  1.8314e-01,\n",
       "         1.6827e-02,  2.6008e-02,  1.2740e-01,  4.8814e-02,  2.5082e-01,\n",
       "         1.8024e-01,  1.6315e-01,  1.4931e-01,  2.9848e-01,  2.6476e-02],\n",
       "       grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write your code\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to add the logistic regression head. Complement the `forward()` method so that it concatenates, $\\mathbf{u}$, $\\mathbf{v}$, and $\\mathbf{|u - v|}$ and outputs three classes. You have only two lines to add."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the forward() method\n",
    "class SBERT(nn.Module):\n",
    "    def __init__(self, nbr_classes, glove, d_model=50, nhead=5, num_layers=4):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embeddings = nn.Embedding(glove.size()[0],\n",
    "                                       glove.size()[1],\n",
    "                                       padding_idx=0).from_pretrained(glove)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model, nhead)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            self.encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(3 * d_model, nbr_classes)\n",
    "\n",
    "    def forward(self, u, v):\n",
    "        ...\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert = SBERT(3, glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SBERT(\n",
       "  (embeddings): Embedding(400003, 50)\n",
       "  (encoder_layer): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=50, out_features=50, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=50, out_features=2048, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=2048, out_features=50, bias=True)\n",
       "    (norm1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=50, out_features=50, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=50, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=50, bias=True)\n",
       "        (norm1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=150, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbert.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([   10,   902,    16,    10,  2870, 11073,    77,    10,  2324,   138,\n",
       "          7353,     5]),\n",
       " tensor([  10,  902,   17,  791,   29, 2870,   13,   10,  994,    5]),\n",
       " tensor(1))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the complete model and we have three outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4798, -0.3071, -0.2568], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbert(dataset[0][0], dataset[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training SBERT\n",
    "We now train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SBERT(\n",
       "  (embeddings): Embedding(400003, 50)\n",
       "  (encoder_layer): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=50, out_features=50, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=50, out_features=2048, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=2048, out_features=50, bias=True)\n",
       "    (norm1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=50, out_features=50, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=50, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=50, bias=True)\n",
       "        (norm1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=150, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbert.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()    # cross entropy loss\n",
    "optimizer = torch.optim.Adam(sbert.parameters(), lr=0.00002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([   10,   902,    16,    10,  2870, 11073,    77,    10,  2324,   138,\n",
       "          7353,     5]),\n",
       " tensor([  10,  902,   17,  791,   29, 2870,   13,   10,  994,    5]),\n",
       " tensor(1))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the training loop. You will process one sample at a time to simplify it i.e. no batch. Record the training loss.\n",
    "\n",
    "A better design would use a `Dataset` object. We will see this construct in the last laboratory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 549367/549367 [1:28:56<00:00, 102.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7725698302376134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 549367/549367 [1:34:03<00:00, 97.35it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6718461502144009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 549367/549367 [1:55:10<00:00, 79.49it/s]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.632829015242741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 549367/549367 [1:58:30<00:00, 77.26it/s]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6079091006910775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 549367/549367 [1:26:30<00:00, 105.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5893157821811403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Write your code\n",
    "ce_train_loss = []\n",
    "for epoch in range(5):\n",
    "    loss_train = 0\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(range(len(ce_train_loss)),\n",
    "            ce_train_loss, c='b', marker='x')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Cross entropy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(sbert, 'sbert_mini.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_saved = torch.load('sbert_mini.pt', weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SBERT(\n",
       "  (embeddings): Embedding(400003, 50)\n",
       "  (encoder_layer): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=50, out_features=50, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=50, out_features=2048, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=2048, out_features=50, bias=True)\n",
       "    (norm1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=50, out_features=50, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=50, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=50, bias=True)\n",
       "        (norm1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=150, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbert_saved.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  10,  902,   17,  791,   29, 2870,   13,   10,  994,    5])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-3.3830,  1.6974, -2.2973], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sbert(dataset[0][0], dataset[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write a function to apply the model to a sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sbert(model, token_indices):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        v = model.embeddings(token_indices)\n",
    "        v = model.transformer_encoder(v).mean(dim=0)\n",
    "        return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.8877, -1.7413, -1.7285,  0.1743, -0.3665, -0.4762, -2.8139,  0.6352,\n",
       "         0.2870,  1.2819, -0.1235, -0.6765,  1.3052, -2.1420, -1.1982, -0.6916,\n",
       "         1.5997,  0.6369, -0.4549,  0.3483,  0.8833,  0.0587, -1.9038,  1.5115,\n",
       "        -1.6933, -0.4778,  0.7484,  0.9568,  0.9330, -1.4335,  2.4563,  0.2131,\n",
       "         0.7525, -0.2026,  1.7107,  0.2447, -0.7956,  0.2265, -0.5726,  0.2223,\n",
       "         0.0867, -0.2678,  0.2863, -0.3720, -0.0434,  0.4855,  1.3381,  0.8416,\n",
       "         1.2122, -0.9992])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_sbert(sbert, dataset[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'entailment', 1: 'neutral', 2: 'contradiction'}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the model to all our pairs and, for each pair, we compute the cosine of the resulting embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 549367/549367 [18:29<00:00, 495.01it/s]\n"
     ]
    }
   ],
   "source": [
    "cos_sim = {'entailment': 0.0, 'neutral': 0.0, 'contradiction': 0.0}\n",
    "cnt = {'entailment': 0.0, 'neutral': 0.0, 'contradiction': 0.0}\n",
    "sbert.eval()\n",
    "\n",
    "for data in tqdm(dataset):\n",
    "    cos_val = compute_cosine(\n",
    "        encode_sbert(sbert, data[0]),\n",
    "        encode_sbert(sbert, data[1]))\n",
    "    cos_sim[idx2label[data[2].item()]] += cos_val\n",
    "    cnt[idx2label[data[2].item()]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'entailment': tensor(132409.5781),\n",
       "  'neutral': tensor(119825.4531),\n",
       "  'contradiction': tensor(94871.9688)},\n",
       " {'entailment': 183416.0, 'neutral': 182764.0, 'contradiction': 183187.0})"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim, cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entailment': tensor(0.7219),\n",
       " 'neutral': tensor(0.6556),\n",
       " 'contradiction': tensor(0.5179)}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for key in cos_sim.keys():\n",
    "    cos_sim[key] /= cnt[key]\n",
    "cos_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the Embedder to Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"The weather is lovely today.\",\n",
    "    \"It's so sunny outside!\",\n",
    "    \"He drove to the stadium.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the', 'weather', 'is', 'lovely', 'today', '.'],\n",
       " ['it', \"'\", 's', 'so', 'sunny', 'outside', '!'],\n",
       " ['he', 'drove', 'to', 'the', 'stadium', '.']]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sents = [tokenize(sent, pattern) for sent in sentences]\n",
    "tokenized_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([    3,  1623,    17, 11130,   376,     5]),\n",
       " tensor([  23,   60, 1537,  103, 9328,  590,  808]),\n",
       " tensor([  21, 3189,    7,    3, 1355,    5])]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexed_sents = [convert_symbols(sent, token2idx) for sent in tokenized_sents]\n",
    "indexed_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([-0.9444, -1.4454, -1.5328, -0.0401,  0.7456,  0.4003,  0.1908,  0.9261,\n",
       "         -0.0935,  1.0552, -0.9348, -0.5338,  0.5411, -0.2320, -1.4104, -0.7052,\n",
       "          0.5182,  1.1477, -0.0579,  0.2766,  2.0090,  0.1287, -0.7176,  0.4973,\n",
       "         -2.1082,  1.0647,  0.3006,  0.8813,  0.5910,  0.7819,  2.4526, -0.9512,\n",
       "          0.5502, -3.7581,  1.2698, -0.4052,  0.9577, -0.1374, -0.1920, -0.0925,\n",
       "          0.1014, -0.1191, -0.1107, -1.3964,  0.9064, -1.1499,  1.3659,  0.8398,\n",
       "         -0.5041, -1.6994]),\n",
       " tensor([ 0.2197, -1.4242, -0.9463,  0.1139,  0.2074, -0.4944, -0.2839,  1.1483,\n",
       "         -0.0717,  0.7992, -1.7604, -0.7386,  0.0365, -0.2602, -2.0892, -0.2036,\n",
       "          0.4227,  0.7776,  0.8279,  0.7327,  0.7355, -0.1863, -0.4514,  1.0287,\n",
       "         -2.0211,  1.4833,  1.6712,  1.1805,  0.8299,  2.1397,  1.8055,  0.1272,\n",
       "          0.6017, -2.4100,  0.7694, -0.6552,  0.3589, -0.6750, -0.2745, -0.1160,\n",
       "         -1.3473,  0.1469, -0.6013, -1.2428,  0.9414, -0.5748,  1.7769,  0.7238,\n",
       "         -1.2138, -1.9278]),\n",
       " tensor([ 0.3263, -1.5300,  0.1388,  0.7091,  0.8850,  0.5430, -1.6405, -1.2144,\n",
       "          1.2593,  1.0906,  0.1994,  0.2288,  0.6783, -0.1360, -1.6335, -0.3695,\n",
       "          0.5525, -0.5259,  0.3810,  1.5417, -0.9392,  2.5488, -1.7343, -0.0574,\n",
       "         -0.3462, -1.0147, -0.1107, -2.2595,  0.5844, -0.0115,  2.2472, -0.0709,\n",
       "          1.1931, -0.6905,  0.3629, -1.6897,  1.2603,  0.0273, -0.6053, -0.5186,\n",
       "          0.5544,  0.1840,  0.3681, -2.5592,  0.9762, -0.1566, -0.6892,  0.1426,\n",
       "          1.2890, -0.4450])]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_sents = [encode_sbert(sbert, sent) for sent in indexed_sents]\n",
    "encoded_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.8306, 0.3039],\n",
       "        [0.8306, 1.0000, 0.2178],\n",
       "        [0.3039, 0.2178, 1.0000]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([compute_cosine(s1, s2)\n",
    "              for s1 in encoded_sents\n",
    "              for s2 in encoded_sents]).reshape(len(sentences), len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turning in your assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now your are done with the program. To complete this assignment, you will write a report where you will:\n",
    "1. Write a short individual report on your program. I recommend that you use this structure for your report:\n",
    "      1. Objectives and dataset\n",
    "      2. Method and program structure, where you should outline your program and possibly describe difficult parts.\n",
    "      3. Results.\n",
    "      4. Conclusion.\n",
    "2. In Sect. _Method and program structure_, do not forget to:\n",
    "   * Summarize the baseline\n",
    "   * Summarize SBERT\n",
    "\n",
    "The whole report should be of 2 to 3 pages.\n",
    "\n",
    "Submit your report as well as your **notebook** (for archiving purposes) to Canvas: https://canvas.education.lu.se/. To write your report, use Latex. This will probably help you structure your text. You can use the Overleaf online editor (www.overleaf.com). You will then upload a PDF file in Canvas.\n",
    "\n",
    "The submission deadline is October 17, 2025."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
